# Mapping Model Assets' Failure Modes to CIA<sup>3</sup>-R Hexagon

| Asset | Properties | Threats | Known attacks|
|:---|:---|:---|:---|
|Hyper-parameters|Confidentiality, Availability, Authorization|Disclosure, DoS, EoP|Nefarious abuse of optimization algorithms by adversaries may lead to erroneous tuning of ML models. Due to their influence over the ML modelsâ€™ predictive capabilities (and, in turn, their commercial value), hyper-parameters are subject to stealing attacks. The literature reports hyper-parameters stealing attacks that target hyper-parameters used to balance between the loss function and the regularization terms in an objective function.|
|Model parameters|Confidentiality, Availability, Authorization|Disclosure, DoS, EoP|In a model extraction attack, an attacker can extract model parameters via querying the ML model. This way, the attacker can build a near-equivalent shadow model that has the same fidelity as that of the original one.|
|Data pre-processing algorithms|Integrity, Availability|Tampering, DoS|Flawed schemata negatively impact on the quality of the ingested information used by applications. An adversary can both compromise the program that pre-process data and mount a schema-based denial of service attack which causes the information necessary for pre-processing to be missing.|
|Trained models|Integrity, Availability|Tampering, DoS|In a model poisoning attack, an attacker can replace a functional and legitimate model file with a poisoned one. This type of attack is more likely to occur in cloud-based ML models by exploiting potential weaknesses of cloud providers.|
|Deployed models|Authenticity, Integrity, Non-repudiation, Confidentiality, Availability, Authorization|Spoofing, Tampering, Repudiation, Disclosure, DoS, EoP|A wide variety of attacks at inference time try to compromise ML deployed models. These include model inversion, model evasion, membership inference, model reuse and exploration attacks, among others.|
