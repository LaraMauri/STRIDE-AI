# Mapping ML Assets' Failure Modes to CIA<sup>3</sup>-R Hexagon

* [Data](https://github.com/LaraMauri/STRIDE-AI/blob/main/pages/mapping-ml-assets-failure-modes-CIA_R.md#data)
* [Models](https://github.com/LaraMauri/STRIDE-AI/blob/main/pages/mapping-ml-assets-failure-modes-CIA_R.md#models)
* [Artefacts](https://github.com/LaraMauri/STRIDE-AI/blob/main/pages/mapping-ml-assets-failure-modes-CIA_R.md#artefacts)

## Data

| Asset | Properties | Threats | Known attacks|
|:---|:---|:---|:---|
|Requirements|Availability|DoS|While no direct attacks to requirements have been reported, unexpected legal liabilities deriving from defective requirements have been described in a number of concrete cases, including ML models for medical diagnostics.|
|Raw data|Authenticity,<br />Confidentiality,<br />Availability,<br />Authorization|Spoofing,<br />Disclosure,<br />DoS,<br />EoP|Attacks by data owners introduce selection bias on purpose when publishing raw data in order to affect inference to be drawn on the data. Reported examples include companies who release biased raw data with the hope competitors would use it to train ML models, causing competitors to diminish the quality of their own products and consumer confidence in them. In perturbation-style attacks, the attacker stealthily modifies raw data to get a desired response from a production-deployed model. This compromises the model’s classification accuracy.|
|Pre-processed data|Integrity,<br />Availability|Tampering,<br />DoS|Attacks that occur in the pre-processing stage (especially in the context of applications processing images) may mislead all subsequent steps in an AI-ML life-cycle. As an example, _image-scaling_ attacks allow attackers to manipulate images so that their appearance changes when scaled to a specific size.|
|Labeled data|Authenticity,<br />Integrity|Spoofing,<br />Tampering|Append attacks target availability by adding random samples to the training set to the point of preventing any model trained on that data set from computing any meaningful inference. Other modifications to the training data set (backdoor or insert attacks) jeopardize the ML model’s integrity by trying to introduce spurious inferences. Attackers randomly draw new labels for a part of the training pool to add an invisible watermark that can later be used to “backdoor” into the model.|
|Augmented data|Integrity,<br />Availability|Tampering,<br />DoS|Adversarial data items tailored to compromise ML model inference can be inserted during data augmentation, in order to make them difficult to detect.|
|Validation data|Integrity,<br />Availability|Tampering,<br />DoS|Attacks  can shorten the training of the ML model by compromising just a small fraction of the validation data set. "Adversarial" training data generated by these attacks are quite different from genuine training set data.|
|Held-out test cases|Integrity,<br />Confidentiality,<br />Availability|Tampering,<br />Disclosure, DoS|Evaluating an ML model’s performance on  HTCs involves reducing all of the information contained in the HTCs outputs to a single number expressing accuracy. The literature reports _slicing attacks_, which poison the held-out data set to produce misleading results. Slicing attacks introduce specific slices of data that doctor the model’s accuracy, making it very different from how it performs on the in-production data set.
|Inferences|Authenticity,<br />Integrity,<br />Availability,<br />Authorization|Spoofing,<br />Tampering,<br />DoS,<br />EoP|Inferences need to carry informative content. The literature reports _eavesdropping attacks_ to distributed ML models involving eavesdropping on inferences.|

## Models

| Asset | Properties | Threats | Known attacks|
|:---|:---|:---|:---|
|Hyper-parameters|Confidentiality,<br />Availability,<br />Authorization|Disclosure,<br />DoS,<br />EoP|Nefarious abuse of optimization algorithms by adversaries may lead to erroneous tuning of ML models. Due to their influence over the ML models’ predictive capabilities (and, in turn, their commercial value), hyper-parameters are subject to stealing attacks. The literature reports hyper-parameters stealing attacks that target hyper-parameters used to balance between the loss function and the regularization terms in an objective function.|
|Model parameters|Confidentiality,<br />Availability,<br />Authorization|Disclosure,<br />DoS,<br />EoP|In a model extraction attack, an attacker can extract model parameters via querying the ML model. This way, the attacker can build a near-equivalent shadow model that has the same fidelity as that of the original one.|
|Data pre-processing algorithms|Integrity,<br />Availability|Tampering,<br />DoS|Flawed schemata negatively impact on the quality of the ingested information used by applications. An adversary can both compromise the program that pre-process data and mount a schema-based denial of service attack which causes the information necessary for pre-processing to be missing.|
|Trained models|Integrity,<br />Availability|Tampering,<br />DoS|In a model poisoning attack, an attacker can replace a functional and legitimate model file with a poisoned one. This type of attack is more likely to occur in cloud-based ML models by exploiting potential weaknesses of cloud providers.|
|Deployed models|Authenticity,<br />Integrity,<br />Non-repudiation,<br />Confidentiality,<br />Availability,<br />Authorization|Spoofing,<br />Tampering,<br />Repudiation,<br />Disclosure,<br />DoS,<br />EoP|A wide variety of attacks at inference time try to compromise ML deployed models. These include model inversion, model evasion, membership inference, model reuse and exploration attacks, among others.|

## Artefacts

| Asset | Properties | Threats | Known attacks|
|:---|:---|:---|:---|
|Model architecture|Authenticity,<br />Confidentiality|Spoofing,<br />Disclosure|Man-in-the-Middle attacks use knowledge of the pipeline structure and interfaces to inject malicious data tailored to maximise damage.|
|Model hardware design|Confidentiality|Disclosure|Side-Channel attacks to ML hardware implementations attacks use physical observation of the hardware operation to  estimate parameters of the ML model implemented by the circuit.|
|Data and metadata schemata|Confidentiality|Disclosure|Model inversion attacks exploit knowledge about a model’s input features (e.g. representation interval, types) for carry out extraction of the original training set data from the model.|
|Learned data indexes|Integrity|Tampering|Index poisoning attacks target the learned index’s data probability distributions to imperceptibly degrade the index performance.|
