# Mapping ML Assets' Failure Modes to CIA<sup>3</sup>-R Hexagon

* [Data](https://github.com/LaraMauri/STRIDE-AI/blob/main/pages/mapping-data-assets-failure-modes-CIA_R.md#data)
* [Models]()
* [Artefacts]()

## Data

| Asset | Properties | Threats | Known attacks|
|:---|:---|:---|:---|
|Requirements|Availability|DoS|While no direct attacks to requirements have been reported, unexpected legal liabilities deriving from defective requirements have been described in a number of concrete cases, including ML models for medical diagnostics.|
|Raw data|Authenticity,<br />Confidentiality,<br />Availability,<br />Authorization|Spoofing,<br />Disclosure,<br />DoS,<br />EoP|Attacks by data owners introduce selection bias on purpose when publishing raw data in order to affect inference to be drawn on the data. Reported examples include companies who release biased raw data with the hope competitors would use it to train ML models, causing competitors to diminish the quality of their own products and consumer confidence in them. In perturbation-style attacks, the attacker stealthily modifies raw data to get a desired response from a production-deployed model. This compromises the model’s classification accuracy.|
|Pre-processed data|Integrity,<br />Availability|Tampering,<br />DoS|Attacks that occur in the pre-processing stage (especially in the context of applications processing images) may mislead all subsequent steps in an AI-ML life-cycle. As an example, _image-scaling_ attacks allow attackers to manipulate images so that their appearance changes when scaled to a specific size.|
|Labeled data|Authenticity,<br />Integrity|Spoofing,<br />Tampering|Append attacks target availability by adding random samples to the training set to the point of preventing any model trained on that data set from computing any meaningful inference. Other modifications to the training data set (backdoor or insert attacks) jeopardize the ML model’s integrity by trying to introduce spurious inferences. Attackers randomly draw new labels for a part of the training pool to add an invisible watermark that can later be used to “backdoor” into the model.|
|Augmented data|Integrity,<br />Availability|Tampering,<br />DoS|Adversarial data items tailored to compromise ML model inference can be inserted during data augmentation, in order to make them difficult to detect.|
|Validation data|Integrity,<br />Availability|Tampering,<br />DoS|Attacks  can shorten the training of the ML model by compromising just a small fraction of the validation data set. "Adversarial" training data generated by these attacks are quite different from genuine training set data.|
|Held-out test cases|Integrity,<br />Confidentiality,<br />Availability|Tampering,<br />Disclosure, DoS|Evaluating an ML model’s performance on  HTCs involves reducing all of the information contained in the HTCs outputs to a single number expressing accuracy. The literature reports _slicing attacks_, which poison the held-out data set to produce misleading results. Slicing attacks introduce specific slices of data that doctor the model’s accuracy, making it very different from how it performs on the in-production data set.
|Inferences|Authenticity,<br />Integrity,<br />Availability,<br />Authorization|Spoofing,<br />Tampering,<br />DoS,<br />EoP|Inferences need to carry informative content. The literature reports _eavesdropping attacks_ to distributed ML models involving eavesdropping on inferences.|
